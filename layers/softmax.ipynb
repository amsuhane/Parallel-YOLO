{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "softmax.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kd8kzECeCulj",
        "colab_type": "code",
        "outputId": "92ea6fe0-e91f-431c-a671-13bb273aa9f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "!pip install git+git://github.com/andreinechaev/nvcc4jupyter.git"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning git://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-aokc6k3h\n",
            "  Running command git clone -q git://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-aokc6k3h\n",
            "Requirement already satisfied (use --upgrade to upgrade): NVCCPlugin==0.0.2 from git+git://github.com/andreinechaev/nvcc4jupyter.git in /usr/local/lib/python3.6/dist-packages\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-cp36-none-any.whl size=4307 sha256=aa97a1d9afc50e3ab71da5cf50dd40c7ff157cd245464aecd8b8209bc12eaad8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-k3mxu6oa/wheels/10/c2/05/ca241da37bff77d60d31a9174f988109c61ba989e4d4650516\n",
            "Successfully built NVCCPlugin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vdLM3C1CyRc",
        "colab_type": "code",
        "outputId": "f788e0c5-2c01-437e-de9d-7007a8ee3de0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%load_ext nvcc_plugin"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The nvcc_plugin extension is already loaded. To reload it, use:\n",
            "  %reload_ext nvcc_plugin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okZAedcfC4Xn",
        "colab_type": "code",
        "outputId": "196bdc25-d02c-4b8e-b8a8-bc2ce5aac101",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%cuda --name softmax.cu\n",
        "#include <cstdio>\n",
        "#include <cstdlib>\n",
        "#include <cmath>\n",
        "#include <ctime>\n",
        "#include <cfloat>\n",
        "\n",
        "#include <algorithm>\n",
        "#include <chrono>\n",
        "#include <iomanip>\n",
        "#include <iostream>\n",
        "#include <map>\n",
        "#include <memory>\n",
        "#include <random>\n",
        "#include <sstream>\n",
        "#include <string>\n",
        "#include <vector>\n",
        "\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#include <device_launch_parameters.h>\n",
        "#include <cublas_v2.h>\n",
        "#include <cudnn.h>\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "/*** Definitions ***/\n",
        "// Block width for CUDA kernels\n",
        "#define BW 128\n",
        "#define RANDOM_SEED -1\n",
        "\n",
        "#ifdef USE_GFLAGS\n",
        "    #include <gflags/gflags.h>\n",
        "\n",
        "    #ifndef _WIN32\n",
        "        #define gflags google\n",
        "    #endif\n",
        "#else\n",
        "    // Constant versions of gflags\n",
        "    #define DEFINE_int32(flag, default_value, description) const int FLAGS_##flag = (default_value)\n",
        "    #define DEFINE_uint64(flag, default_value, description) const unsigned long long FLAGS_##flag = (default_value)\n",
        "    #define DEFINE_bool(flag, default_value, description) const bool FLAGS_##flag = (default_value)\n",
        "    #define DEFINE_double(flag, default_value, description) const double FLAGS_##flag = (default_value)\n",
        "    #define DEFINE_string(flag, default_value, description) const std::string FLAGS_##flag ((default_value))\n",
        "#endif\n",
        "\n",
        "#define FatalError(s) do {                                             \\\n",
        "    std::stringstream _where, _message;                                \\\n",
        "    _where << __FILE__ << ':' << __LINE__;                             \\\n",
        "    _message << std::string(s) + \"\\n\" << __FILE__ << ':' << __LINE__;  \\\n",
        "    std::cerr << _message.str() << \"\\nAborting...\\n\";                  \\\n",
        "    cudaDeviceReset();                                                 \\\n",
        "    exit(1);                                                           \\\n",
        "} while(0)\n",
        "\n",
        "#define checkCUDNN(status) do {                                        \\\n",
        "    std::stringstream _error;                                          \\\n",
        "    if (status != CUDNN_STATUS_SUCCESS) {                              \\\n",
        "      _error << \"CUDNN failure: \" << cudnnGetErrorString(status);      \\\n",
        "      FatalError(_error.str());                                        \\\n",
        "    }                                                                  \\\n",
        "} while(0)\n",
        "\n",
        "#define checkCudaErrors(status) do {                                   \\\n",
        "    std::stringstream _error;                                          \\\n",
        "    if (status != 0) {                                                 \\\n",
        "      _error << \"Cuda failure: \" << status;                            \\\n",
        "      FatalError(_error.str());                                        \\\n",
        "    }                                                                  \\\n",
        "} while(0)\n",
        "\n",
        "\n",
        "class Softmax \n",
        "/*Expected Input Tensor Shape [N,1,1,W] in NCHW format in constructor\n",
        "N = BATCH_SIZE, W = flattened vector length\n",
        "Output is of same shape out = softmax(inp) \n",
        "backprop expects dL/dout (grad_in) and returns dL/dinp\n",
        "L = any loss computed from output of softmax eg cross entropy \n",
        "Note that we need to have another layer to compute loss if we like to calculate*/\n",
        "{\n",
        "    public:\n",
        " \n",
        "        const float alpha = 1.0f;\n",
        "        const float beta = 0.0f;\n",
        "        \n",
        "        cudnnTensorDescriptor_t input_descriptor;\n",
        "        cudnnTensorDescriptor_t output_descriptor;\n",
        "        \n",
        "        cudnnHandle_t cudnn;\n",
        "        cublasHandle_t cublas;\n",
        " \n",
        "        /*** These variables will be on GPU as cache for backward pass ***/\n",
        "        float *dot;  //Output of softmax i.e., dot = softmax(d_input) in forward, necessary to cache for backward\n",
        " \n",
        "        /*** These variables will be on CPU ***/\n",
        "        int input_size, output_size;\n",
        "        int out_height, out_width;\n",
        "        int in_channels, out_channels;\n",
        "        int gpu_id;\n",
        "        float *dot_cpu; //Cache for backprop\n",
        " \n",
        "        Softmax(int _in_channels, int _out_channels, cudnnHandle_t _cudnn, cublasHandle_t _cublas,\n",
        "             int batch_size, int height, int width, int _gpu_id)\n",
        "        {\n",
        "            cudnn = _cudnn;\n",
        "            cublas = _cublas;\n",
        "            gpu_id = _gpu_id;\n",
        "\n",
        "            checkCudaErrors(cudaSetDevice(gpu_id));\n",
        "         \n",
        "            in_channels = _in_channels;\n",
        "            out_channels = _out_channels;\n",
        "            out_width = width;\n",
        "            out_height = height;\n",
        "         \n",
        "            checkCUDNN(cudnnCreateTensorDescriptor(&input_descriptor));\n",
        "            checkCUDNN(cudnnSetTensor4dDescriptor(input_descriptor, \n",
        "                                                      CUDNN_TENSOR_NCHW,\n",
        "                                                      CUDNN_DATA_FLOAT,\n",
        "                                                      batch_size,\n",
        "                                                      in_channels,\n",
        "                                                      height,\n",
        "                                                      width));\n",
        "                \n",
        "            checkCUDNN(cudnnCreateTensorDescriptor(&output_descriptor));\n",
        "            checkCUDNN(cudnnSetTensor4dDescriptor(output_descriptor,\n",
        "                                                      CUDNN_TENSOR_NCHW,\n",
        "                                                      CUDNN_DATA_FLOAT,\n",
        "                                                      batch_size,\n",
        "                                                      out_channels,\n",
        "                                                      out_height,\n",
        "                                                      out_width));\n",
        "            \n",
        "            /*** Allocate memory to GPU placeholders ***/\n",
        "            input_size = batch_size * in_channels * height * width;\n",
        "            output_size = input_size; //output_size means output of softmax, not the scalar loss\n",
        "         \n",
        "            checkCudaErrors(cudaMalloc(&dot, sizeof(float) * output_size));\n",
        "            dot_cpu = (float *)malloc(sizeof(float) * output_size);\n",
        "        }\n",
        " \n",
        "        void forward(float *d_input, float *d_output)\n",
        "        {\n",
        "            checkCUDNN(cudnnSoftmaxForward(\n",
        "                cudnn,\n",
        "                CUDNN_SOFTMAX_ACCURATE,\n",
        "                CUDNN_SOFTMAX_MODE_INSTANCE,\n",
        "                &alpha,\n",
        "                input_descriptor,\n",
        "                d_input,\n",
        "                &beta,\n",
        "                output_descriptor,\n",
        "                d_output\n",
        "            ));\n",
        "         \n",
        "            //Store the output of softmax for backprop\n",
        "            checkCudaErrors(cudaMemcpy(dot_cpu, d_output, sizeof(float) * output_size, cudaMemcpyDeviceToHost));\n",
        "            checkCudaErrors(cudaMemcpy(dot, dot_cpu, sizeof(float) * output_size,  cudaMemcpyHostToDevice));\n",
        "        }\n",
        " \n",
        "        void backward(float *grad_above, float *grad_out)\n",
        "        {\n",
        "            checkCUDNN(cudnnSoftmaxBackward(\n",
        "                cudnn,\n",
        "                CUDNN_SOFTMAX_ACCURATE,\n",
        "                CUDNN_SOFTMAX_MODE_INSTANCE,\n",
        "                &alpha,\n",
        "                output_descriptor,\n",
        "                dot,\n",
        "                output_descriptor,\n",
        "                grad_above,\n",
        "                &beta,\n",
        "                input_descriptor,\n",
        "                grad_out\n",
        "            ));\n",
        "        }\n",
        "};\n",
        "\n",
        "void pprint(float *a, int n, int WIDTH)\n",
        "{\n",
        "    for(int i=0; i<n; i++)\n",
        "    {\n",
        "        if(i % WIDTH==0)\n",
        "            cout << \"\\n\";\n",
        "        cout << a[i] << \" \";\n",
        "    }\n",
        "    cout<<endl;\n",
        "}\n",
        "\n",
        "void test_softmax() \n",
        "{\n",
        "    int WIDTH = 5, HEIGHT = 1, BATCH_SIZE = 5, CHANNELS = 1; //Input to softmax is of shape (N,1,1,W)\n",
        "    int GPU_ID = 0;\n",
        "    checkCudaErrors(cudaSetDevice(GPU_ID));\n",
        " \n",
        "    float *data, *dout;\n",
        "    cudnnHandle_t cudnn;\n",
        "    cublasHandle_t cublas;\n",
        "\n",
        "    cudnnCreate(&cudnn);\n",
        "    cublasCreate(&cublas);\n",
        " \n",
        "    Softmax R(CHANNELS, CHANNELS, cudnn, cublas, BATCH_SIZE, HEIGHT, WIDTH, GPU_ID);\n",
        " \n",
        "    cudaMalloc((void **)&data, sizeof(float) * R.input_size);\n",
        "    cudaMalloc((void **)&dout, sizeof(float) * R.output_size);\n",
        " \n",
        "    //cudaMalloc((void **)&dtarget, sizeof(float) * R.output_size);\n",
        "\n",
        "    float *cpu_data = (float *)malloc(sizeof(float) * R.input_size);\n",
        "    //float *cpu_target = (float *)malloc(sizeof(float) * R.input_size);\n",
        "    //float *cpu_loss = (float *)malloc(sizeof(float) * 1);\n",
        "    for(int i = 0; i < R.input_size; i++)\n",
        "    {\n",
        "        cpu_data[i] = i+1.0;\n",
        "    }\n",
        "    cpu_data[5] = 1;\n",
        "    cpu_data[20] = -1;\n",
        " \n",
        "    cout<<\"Testing Softmax forward . . .\"<<endl;\n",
        "    cout << \"Input Matrix:\";\n",
        "    pprint(cpu_data, R.input_size, WIDTH);\n",
        "    //cout<<\"Target :\";\n",
        "    //pprint(cpu_target, R.input_size, WIDTH);\n",
        " \n",
        "    cout << \"\\nApply Softmax:\"<<endl;\n",
        "    checkCudaErrors(cudaMemcpy(data, cpu_data, sizeof(float) * R.input_size,  cudaMemcpyHostToDevice));\n",
        "    //checkCudaErrors(cudaMemcpy(dtarget, cpu_target, sizeof(float) * R.input_size,  cudaMemcpyHostToDevice));\n",
        "    \n",
        "    R.forward(data, dout);\n",
        " \n",
        "    float *out = (float *)malloc(sizeof(float) * R.output_size);\n",
        "    checkCudaErrors(cudaMemcpy(out, dout, sizeof(float) * R.output_size, cudaMemcpyDeviceToHost));\n",
        "    //checkCudaErrors(cudaMemcpy(cpu_loss, dloss, sizeof(float) * R.output_size, cudaMemcpyDeviceToHost));\n",
        "    //cout<<\"Loss = \"<<cpu_loss[0]<<endl;\n",
        "    cout << \"Output Matrix:\";\n",
        "    pprint(out, R.output_size, WIDTH);\n",
        " \n",
        " \n",
        "    cout<<\"Testing Backward . . .\"<<endl;\n",
        "    float *cpu_dup = (float *)malloc(sizeof(float) * R.output_size);\n",
        "    for(int i=0; i<R.output_size; i++)\n",
        "        cpu_dup[i] = 0;\n",
        " \n",
        "    //Remember dL/dy_hat = [0, 0, 0, 0 . . . , -1/y_hat[k], 0, 0, . . ., 0]\n",
        "    cpu_dup[2] = -1 / out[2]; //It means 1st row in Batch had target label at index = 2\n",
        "    cpu_dup[8] = -1 / out[8]; //It means 1st row in Batch had target label at index = 8 and so on\n",
        "    cpu_dup[10] = -1/out[10];\n",
        "    cpu_dup[16] = -1/out[16];\n",
        "    cpu_dup[22] = -1/out[22];\n",
        "\n",
        " \n",
        "    cout << \"Upstream Derivatives:\";\n",
        "    pprint(cpu_dup, R.output_size, WIDTH);\n",
        " \n",
        "    float *dup, *dgrad;\n",
        "    cudaMalloc((void **)&dup, sizeof(float) * R.output_size);\n",
        "    cudaMalloc((void **)&dgrad, sizeof(float) * R.input_size);\n",
        " \n",
        "    checkCudaErrors(cudaMemcpy(dup, cpu_dup, sizeof(float) * R.output_size,  cudaMemcpyHostToDevice));\n",
        "    cout << \"\\nApply Backward:\"<<endl;\n",
        "    R.backward(dup, dgrad);\n",
        " \n",
        "    float *cpu_dout = (float *)malloc(sizeof(float) * R.input_size);\n",
        "    checkCudaErrors(cudaMemcpy(cpu_dout, dgrad, sizeof(float) * R.input_size, cudaMemcpyDeviceToHost));\n",
        "    cout << \"Back prop results (Expected y_hat - y_target for each row):\"<<endl;\n",
        "    pprint(cpu_dout, R.input_size, WIDTH);\n",
        " \n",
        "    cout<<endl;\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "    cout<<\"In main function . . .\"<<endl;\n",
        "    test_softmax();\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'File written in /content/src/softmax.cu'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSptooRCI09z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvcc /content/src/softmax.cu -o /content/src/softmax -lcudnn -lcublas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxr0bOGgI8En",
        "colab_type": "code",
        "outputId": "ff4dbc8c-08b4-49a7-d78b-a22bb5e5768f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        }
      },
      "source": [
        "!/content/src/softmax"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In main function . . .\n",
            "Testing Softmax forward . . .\n",
            "Input Matrix:\n",
            "1 2 3 4 5 \n",
            "1 7 8 9 10 \n",
            "11 12 13 14 15 \n",
            "16 17 18 19 20 \n",
            "-1 22 23 24 25 \n",
            "\n",
            "Apply Softmax:\n",
            "Output Matrix:\n",
            "0.0116562 0.0316849 0.0861285 0.234122 0.636409 \n",
            "7.9459e-05 0.0320561 0.0871374 0.236864 0.643863 \n",
            "0.0116562 0.0316849 0.0861285 0.234122 0.636409 \n",
            "0.0116562 0.0316849 0.0861285 0.234122 0.636409 \n",
            "3.28982e-12 0.0320586 0.0871443 0.236883 0.643914 \n",
            "Testing Backward . . .\n",
            "Upstream Derivatives:\n",
            "0 0 -11.6106 0 0 \n",
            "0 0 0 -4.22183 0 \n",
            "-85.791 0 0 0 0 \n",
            "0 -31.5608 0 0 0 \n",
            "0 0 -11.4752 0 0 \n",
            "\n",
            "Apply Backward:\n",
            "Back prop results (Expected y_hat - y_target for each row):\n",
            "\n",
            "0.0116562 0.0316849 -0.913871 0.234122 0.636409 \n",
            "7.9459e-05 0.0320561 0.0871374 -0.763136 0.643863 \n",
            "-0.988344 0.0316849 0.0861285 0.234122 0.636409 \n",
            "0.0116562 -0.968315 0.0861285 0.234122 0.636409 \n",
            "3.28982e-12 0.0320586 -0.912856 0.236883 0.643914 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDWH0JWIJTzL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}